{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codigos usados por SEPARADO para optimizar los hiperparametros de diferentes algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAMOS RandomizedSearchCV PARA OPTIMIZAR LOS HIPERPARAMETROS DE DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "def eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n",
    "    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n",
    "    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n",
    "    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n",
    "    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) \n",
    "    print(\"AUC: %.2f\" % auc(fpr, tpr))\n",
    "    \n",
    "# List of hyperparameter\n",
    "max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree\n",
    "min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node\n",
    "min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node\n",
    "max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "\n",
    "hyperparameters = dict(max_depth=max_depth, \n",
    "                       min_samples_split=min_samples_split, \n",
    "                       min_samples_leaf=min_samples_leaf,\n",
    "                       max_features=max_features,\n",
    "                       criterion=criterion,\n",
    "                       splitter=splitter\n",
    "                      )\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "model = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='roc_auc')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "eval_classification(model, y_pred, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('Best max_depth:', model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split:', model.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best max_features:', model.best_estimator_.get_params()['max_features'])\n",
    "print('Best criterion:', model.best_estimator_.get_params()['criterion'])\n",
    "print('Best splitter:', model.best_estimator_.get_params()['splitter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAMOS RandomizedSearchCV PARA OPTIMIZAR LOS HIPERPARAMETROS DE DecisionTreeClassifier\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, 100],\n",
    "              \"max_features\": randint(1, 20),\n",
    "              \"min_samples_leaf\": randint(1, 20),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "\n",
    "tree_cv.fit(X,y)\n",
    "\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAMOS RandomizedSearchCV PARA OPTIMIZAR LOS HIPERPARAMETROS DE RandomForestRegressor/RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 35)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 20, num = 20)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 120, num = 12)]\n",
    "min_samples_split = [2, 6, 10]\n",
    "min_samples_leaf = [1, 3, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "'max_features': max_features,\n",
    "\n",
    "'max_depth': max_depth,\n",
    "\n",
    "'min_samples_split': min_samples_split,\n",
    "\n",
    "'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "\n",
    "param_distributions = random_grid,\n",
    "               n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)\n",
    "rf_random.fit(X,y)\n",
    "\n",
    "\n",
    "print ('Random grid: ', random_grid, '\\n')\n",
    "print ('Best Parameters: ', rf_random.best_params_, ' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAMOS GridSearchCV PARA OPTIMIZAR LOS HIPERPARAMETROS DE DecisionTreeClassifier y Pca para Ver la cantidad de componentes\n",
    "\n",
    "std_slc = StandardScaler()\n",
    "pca = decomposition.PCA()\n",
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                           ('pca', pca),\n",
    "                            ('dec_tree', dec_tree)])\n",
    "\n",
    "n_components = list(range(1,X.shape[1]+1,1))\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12,15,18,20,25,30,35,40]\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth)\n",
    "\n",
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X, y)        \n",
    "\n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAMOS bayes PARA OPTIMIZAR LOS HIPERPARAMETROS DE XGBClassifier\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    clf=xgb.XGBClassifier(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']))\n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"auc\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)\n",
    "\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
